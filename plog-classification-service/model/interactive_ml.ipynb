{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recyclability Classifier, based on OpenAI's CLIP Model\n",
    "Source: https://openai.com/research/clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import torch\n",
    "import clip\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df_dataset = load_from_pickle(dataset_file)\n",
    "x_train, x_test, text_prompt, y_test = train_test_split(df_dataset['File'], df_dataset['Material Class'], test_size=0.2, stratify=df_dataset['Material Class'], random_state=1234)\n",
    "zs_x = df_dataset['File']\n",
    "zs_y = df_dataset['Material Class']\n",
    "\n",
    "# Initialise material classes\n",
    "material_classes = list(material_class_mapping.values())\n",
    "\n",
    "# Preparations for model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "text_prompt = torch.cat([clip.tokenize(f\"a photo of an object made of {c}\") for c in material_classes]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RN50\n",
      "Model parameters: 102,007,137\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "\n",
      "Model: RN101\n",
      "Model parameters: 119,688,033\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "\n",
      "Model: RN50x4\n",
      "Model parameters: 178,300,601\n",
      "Input resolution: 288\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "\n",
      "Model: RN50x16\n",
      "Model parameters: 290,979,217\n",
      "Input resolution: 384\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "\n",
      "Model: RN50x64\n",
      "Model parameters: 623,258,305\n",
      "Input resolution: 448\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "\n",
      "Model: ViT-B/32\n",
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "\n",
      "Model: ViT-B/16\n",
      "Model parameters: 149,620,737\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "\n",
      "Model: ViT-L/14\n",
      "Model parameters: 427,616,513\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "\n",
      "Model: ViT-L/14@336px\n",
      "Model parameters: 427,944,193\n",
      "Input resolution: 336\n",
      "Context length: 77\n",
      "Vocab size: 49408\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare all available models\n",
    "for current_model in clip.available_models():\n",
    "    # Initialise predictions\n",
    "    y_train = [0] * len(text_prompt)\n",
    "    y_test_pred = [0] * len(y_test)\n",
    "    \n",
    "    # Get model and its specifications\n",
    "    model, preprocess = clip.load(current_model, device=device, download_root=MODEL_FOLDER)\n",
    "    input_resolution = model.visual.input_resolution\n",
    "    context_length = model.context_length\n",
    "    vocab_size = model.vocab_size\n",
    "\n",
    "    print(\"Model:\", current_model)\n",
    "    print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "    print(\"Input resolution:\", input_resolution)\n",
    "    print(\"Context length:\", context_length)\n",
    "    print(\"Vocab size:\", vocab_size, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise predictions\n",
    "y_train = [0] * len(text_prompt)\n",
    "y_test_pred = [0] * len(y_test)\n",
    "zs_y_pred = [0] * len(zs_y)\n",
    "\n",
    "\"\"\" Zero Shot Learning \"\"\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device, download_root=MODEL_FOLDER)\n",
    "for current_image in zs_x:\n",
    "    # Initialise image\n",
    "    image = preprocess(Image.open(current_image)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Classify image's material type\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(text_prompt)\n",
    "        \n",
    "        # logits_per_image, logits_per_text = model(image, text_prompt)\n",
    "        # probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "        values, indices = similarity[0].topk(len(material_classes))\n",
    "\n",
    "        # Add result to predictions\n",
    "        y_train.append(int(indices[0]))\n",
    "\n",
    "        # Print the result\n",
    "        # print(\"\\nTop predictions:\\n\")\n",
    "        # for value, index in zip(values, indices):\n",
    "        #     print(f\"{material_classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_train contains the true class labels and y_pred contains the predicted class labels\n",
    "# Example usage:\n",
    "y_train = [0, 1, 2, 2, 0, 1]\n",
    "y_pred = [0, 1, 1, 2, 1, 0]\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_train, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(y_train, y_pred, average='macro')\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(y_train, y_pred, average='macro')\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# F1 Score\n",
    "f1 = f1_score(y_train, y_pred, average='macro')\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Classification Report\n",
    "class_report = classification_report(y_train, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
