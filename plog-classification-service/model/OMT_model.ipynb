{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Material Type (OMT) Classifier, based on OpenAI's CLIP Model\n",
    "Source: https://openai.com/research/clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common import *\n",
    "from model_functions import *\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import clip\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df_dataset = load_from_pickle(dataset_file)\n",
    "\n",
    "# 80-20 Train-Test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_dataset['File'], df_dataset['Material Class'], test_size=0.2, stratify=df_dataset['Material Class'], random_state=1234)\n",
    "\n",
    "# Initialise material classes\n",
    "material_classes = [i.lower() for i in material_class_mapping.values()]\n",
    "for idx, i in enumerate(material_classes):\n",
    "    if(i == 'others'):\n",
    "        material_classes[idx] = \"anything other than paper, plastic, glass, or metal\"\n",
    "\n",
    "# Initialise material class text prompt mapping\n",
    "material_class_prompt_mapping = {}\n",
    "for idx, i in enumerate(material_classes):\n",
    "    material_class_prompt_mapping[idx] = f\"a photo of an object made of {i}\"\n",
    "inverse_material_class_prompt_mapping = {v: k for k, v in material_class_prompt_mapping.items()}\n",
    "\n",
    "# Select computation device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load model (ensure jit=False for training)\n",
    "model, preprocess = clip.load(optimal_model, device=device, download_root=MODEL_FOLDER, jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class image_title_dataset():\n",
    "    def __init__(self, list_image_path,list_txt):\n",
    "        # Initialize image paths and corresponding texts\n",
    "        self.image_path = list_image_path\n",
    "        # Tokenize text using CLIP's tokenizer\n",
    "        self.title  = clip.tokenize(list_txt)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        image = preprocess(Image.open(self.image_path[idx]))\n",
    "        # image = preprocess(Image.open(self.image_path[idx])).unsqueeze(0).to(device)\n",
    "        \n",
    "        title = self.title[idx]\n",
    "        return image, title\n",
    "\n",
    "# Create custom OMT dataset\n",
    "dataset = image_title_dataset(list(x_train), list(y_train.map(material_class_prompt_mapping)))\n",
    "\n",
    "# Define dataloader to efficient load dataset in batches during training\n",
    "train_dataloader = DataLoader(dataset, batch_size=1000, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert model's parameters to FP32 format\n",
    "def convert_models_to_fp32(model): \n",
    "    for p in model.parameters(): \n",
    "        p.data = p.data.float() \n",
    "        p.grad.data = p.grad.data.float() \n",
    "\n",
    "if device == \"cpu\":\n",
    "  model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) # the lr is smaller, more safe for fine tuning to new dataset\n",
    "\n",
    "# Specify the loss function\n",
    "loss_img = nn.CrossEntropyLoss()\n",
    "loss_txt = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # In each iteration, we load a batch of images and their corresponding captions.\n",
    "        images, texts = batch \n",
    "        \n",
    "        # The data is passed through our model, generating predictions.\n",
    "        images = images.to(device)\n",
    "        texts = texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        \n",
    "        # These predictions are compared with the ground truth to calculate the loss.\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
    "        total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
    "        \n",
    "        # This loss is then back-propagated through the network to update the modelâ€™s parameters.\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        if device == \"cpu\":\n",
    "            optimizer.step()\n",
    "        else: \n",
    "            convert_models_to_fp32(model)\n",
    "            optimizer.step()\n",
    "            clip.model.convert_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model\n",
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': total_loss,\n",
    "        }, model_file) #just change to your preferred folder/filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model, preprocess = clip.load(optimal_model, device=device, download_root=MODEL_FOLDER, jit=False)\n",
    "checkpoint = torch.load(model_file)\n",
    "\n",
    "# Use these 3 lines if you use default model setting(not training setting) of the clip. For example, if you set context_length to 100 since your string is very long during training, then assign 100 to checkpoint['model_state_dict'][\"context_length\"] \n",
    "checkpoint['model_state_dict'][\"input_resolution\"] = model.input_resolution #default is 224\n",
    "checkpoint['model_state_dict'][\"context_length\"] = model.context_length # default is 77\n",
    "checkpoint['model_state_dict'][\"vocab_size\"] = model.vocab_size \n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initial Performance \"\"\"\n",
    "# # Get model specifications\n",
    "# input_resolution = model.visual.input_resolution\n",
    "# context_length = model.context_length\n",
    "# vocab_size = model.vocab_size\n",
    "\n",
    "# print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "# print(\"Input resolution:\", input_resolution)\n",
    "# print(\"Context length:\", context_length)\n",
    "# print(\"Vocab size:\", vocab_size, end='\\n\\n')\n",
    "\n",
    "# # Initialise predictions\n",
    "# zs_y_pred = []\n",
    "# for current_image in zs_x:\n",
    "#     # Resize image and center-crop it to conform with the image resolution that the model expects\n",
    "#     image = preprocess(Image.open(current_image)).unsqueeze(0).to(device)\n",
    "\n",
    "#     # Classify image's material type\n",
    "#     with torch.no_grad(): # Disable gradient calculation as we're only perfoming inference\n",
    "#         image_features = model.encode_image(image)\n",
    "#         text_features = model.encode_text(text_prompt)\n",
    "        \n",
    "#         # logits_per_image, logits_per_text = model(image, text_prompt)\n",
    "#         # probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "#         # Normalise features\n",
    "#         image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "#         text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "#         # Obtain cosine similarity using matrix multiplication (@), and apply softmax to get probability distribution over classes\n",
    "#         similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "#         values, indices = similarity[0].topk(len(material_classes))\n",
    "\n",
    "#         # Add result to predictions\n",
    "#         zs_y_pred.append(int(indices[0]))\n",
    "\n",
    "#         # Print the result\n",
    "#         # print(\"\\nTop predictions:\\n\")\n",
    "#         # for value, index in zip(values, indices):\n",
    "#         #     print(f\"{material_classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
    "\n",
    "# # Get model performance\n",
    "# results = multi_class_metrics(list(zs_y), zs_y_pred)\n",
    "# accuracy = results['accuracy']\n",
    "# precision = results['precision']\n",
    "# recall = results['recall']\n",
    "# f1 = results['f1']\n",
    "# mcc = results['mcc']\n",
    "# kappa = results['kappa']\n",
    "# hamming_loss_val = results['hamming_loss_val']\n",
    "# cm = results['cm']\n",
    "# class_report = results['class_report']\n",
    "\n",
    "# # Print results\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "# print(\"Precision:\", precision)\n",
    "# print(\"Recall:\", recall)\n",
    "# print(\"F1 Score:\", f1)\n",
    "# print(\"Matthews Correlation Coefficient (MCC):\", mcc)\n",
    "# print(\"Cohen's Kappa:\", kappa)\n",
    "# print(\"Hamming Loss:\", hamming_loss_val, end='\\n\\n')\n",
    "# print(\"Confusion Matrix:\\n\", cm, end=\"\\n\\n\")\n",
    "# print(\"Classification Report:\\n\", class_report, end=\"\\n\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
